{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9b6345b",
   "metadata": {},
   "source": [
    "# Stock Market Prediction Project\n",
    "## Part 1: Support Vector Classifier for Stock Buying Decisions\n",
    "## Part 2: ARIMA, SVR, XGBoost, and Neural Networks for Stock Price Prediction\n",
    "\n",
    "**Project Overview:**\n",
    "- Analyze 10 stocks from different industries\n",
    "- Implement SVC for buying decisions\n",
    "- Compare SARIMAX, SVR, XGBoost, and Neural Networks for price prediction\n",
    "- Create comprehensive visualizations using Plotly\n",
    "- Build equal-weight portfolio and compare with market indices\n",
    "\n",
    "**Stocks Analyzed:**\n",
    "1. HDFC Bank (Banking)\n",
    "2. TCS (IT Services)\n",
    "3. Maruti Suzuki (Automobile)\n",
    "4. Asian Paints (Manufacturing)\n",
    "5. Dabur (FMCG)\n",
    "6. Dr. Reddy's (Healthcare)\n",
    "7. Apollo Hospitals (Healthcare)\n",
    "8. Airtel (Telecom)\n",
    "9. Mazagon Dock (Shipyard)\n",
    "10. Motilal Oswal (NBFC)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "320c3f61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\home\\miniconda3\\python.exe\n",
      "3.9.5 (default, May 18 2021, 14:42:02) [MSC v.1916 64 bit (AMD64)]\n",
      "^C\n",
      "^C\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mgraphics\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtsaplots\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m plot_acf, plot_pacf\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mstatsmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiagnostic\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m acorr_ljungbox\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Sequential\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LSTM, Dense, Dropout, GRU\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "# Importing all necessary libraries\n",
    "import sys\n",
    "print(sys.executable)\n",
    "print(sys.version)\n",
    "!{sys.executable} -m pip install tensorflow --timeout=120\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.svm import SVC, SVR\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost as xgb\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "pyo.init_notebook_mode(connected=True)\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import glob\n",
    "\n",
    "print(\"All libraries imported successfully!\")\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9ddfd5",
   "metadata": {},
   "source": [
    "## Data Loading and Preprocessing\n",
    "\n",
    "In this section, we will:\n",
    "1. Load all 10 stock datasets\n",
    "2. Clean and standardize the data format\n",
    "3. Handle missing values and outliers\n",
    "4. Create consistent date indexing\n",
    "5. Prepare data for both classification (buying decisions) and regression (price prediction) tasks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54058d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stocks_info = {\n",
    "    'HDFC_BANK.csv': {'name': 'HDFC Bank', 'sector': 'Banking'},\n",
    "    'TCS_IT.csv': {'name': 'TCS', 'sector': 'IT Services'},\n",
    "    'Maruti_automobile.csv': {'name': 'Maruti Suzuki', 'sector': 'Automobile'},\n",
    "    'Asianpaints_manufacturing.csv': {'name': 'Asian Paints', 'sector': 'Manufacturing'},\n",
    "    'Dabur_fmcg.csv': {'name': 'Dabur', 'sector': 'FMCG'},\n",
    "    'Dr.reddy_healthcare.csv': {'name': 'Dr. Reddy\\'s', 'sector': 'Healthcare'},\n",
    "    'APOLLO_Hospitals.csv': {'name': 'Apollo Hospitals', 'sector': 'Healthcare'},\n",
    "    'Airtel_telecom.csv': {'name': 'Airtel', 'sector': 'Telecom'},\n",
    "    'Mazagon_shipyard.csv': {'name': 'Mazagon Dock', 'sector': 'Shipyard'},\n",
    "    'Motilal_oswal.csv': {'name': 'Motilal Oswal', 'sector': 'NBFC'}\n",
    "}\n",
    "\n",
    "\n",
    "def load_stock_data(filename, stock_name, sector):\n",
    "    try:\n",
    "        df = pd.read_csv(filename)\n",
    "        \n",
    "\n",
    "        print(f\"\\n=== {stock_name} ({sector}) ===\")\n",
    "        print(f\"Original shape: {df.shape}\")\n",
    "        print(f\"Columns: {list(df.columns)}\")\n",
    "        \n",
    "        df.columns = df.columns.str.strip()\n",
    "        \n",
    "        column_mapping = {\n",
    "            'Date': 'Date',\n",
    "            'Open Price': 'OPEN',\n",
    "            'High Price': 'HIGH', \n",
    "            'Low Price': 'LOW',\n",
    "            'Close Price': 'CLOSE',\n",
    "            'Last Price': 'CLOSE',  \n",
    "            'Total Traded Quantity': 'VOLUME'\n",
    "        }\n",
    "        \n",
    "        for old_col, new_col in column_mapping.items():\n",
    "            if old_col in df.columns and new_col not in df.columns:\n",
    "                df[new_col] = df[old_col]\n",
    "        \n",
    "        \n",
    "        required_cols = ['Date', 'OPEN', 'HIGH', 'LOW', 'CLOSE']\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        \n",
    "        if missing_cols:\n",
    "            print(f\"Warning: Missing columns {missing_cols} in {filename}\")\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        df['Date'] = pd.to_datetime(df['Date'], format='%d-%b-%y', errors='coerce')\n",
    "        \n",
    "        \n",
    "        df = df.dropna(subset=['Date'])\n",
    "        \n",
    "        \n",
    "        price_cols = ['OPEN', 'HIGH', 'LOW', 'CLOSE']\n",
    "        for col in price_cols:\n",
    "            if col in df.columns:\n",
    "                df[col] = df[col].astype(str).str.replace(',', '').astype(float)\n",
    "        \n",
    "       \n",
    "        df = df.sort_values('Date').reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        df = df.drop_duplicates(subset=['Date']).reset_index(drop=True)\n",
    "        \n",
    "        \n",
    "        df['STOCK_NAME'] = stock_name\n",
    "        df['SECTOR'] = sector\n",
    "        \n",
    "\n",
    "        df['RETURNS'] = df['CLOSE'].pct_change()\n",
    "        df['VOLATILITY'] = df['RETURNS'].rolling(window=20).std()\n",
    "        df['SMA_20'] = df['CLOSE'].rolling(window=20).mean()\n",
    "        df['SMA_50'] = df['CLOSE'].rolling(window=50).mean()\n",
    "        \n",
    "        print(f\"Processed shape: {df.shape}\")\n",
    "        print(f\"Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "        print(f\"Missing values: {df[price_cols].isnull().sum().sum()}\")\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {filename}: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "stocks_data = {}\n",
    "for filename, info in stocks_info.items():\n",
    "    if os.path.exists(filename):\n",
    "        df = load_stock_data(filename, info['name'], info['sector'])\n",
    "        if df is not None:\n",
    "            stocks_data[info['name']] = df\n",
    "    else:\n",
    "        print(f\"File not found: {filename}\")\n",
    "\n",
    "print(f\"\\nSuccessfully loaded {len(stocks_data)} stocks\")\n",
    "print(\"Available stocks:\", list(stocks_data.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "028c6863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preparation for Machine Learning Models\n",
    "\n",
    "def prepare_features_and_target(df, stock_name):\n",
    "    df_processed = df.copy()\n",
    "    df_processed.index = pd.to_datetime(df_processed['Date'])\n",
    "    df_processed = df_processed.drop(['Date'], axis='columns')\n",
    "    df_processed['Open-Close'] = df_processed['OPEN'] - df_processed['CLOSE']\n",
    "    df_processed['High-Low'] = df_processed['HIGH'] - df_processed['LOW']\n",
    "    X = df_processed[['Open-Close', 'High-Low']].copy()\n",
    "    y = np.where(df_processed['CLOSE'].shift(-1) > df_processed['CLOSE'], 1, 0)\n",
    "    X = X[:-1]\n",
    "    y = y[:-1]\n",
    "    \n",
    "    print(f\"\\n{stock_name} - Features and Target prepared:\")\n",
    "    print(f\"Features shape: {X.shape}\")\n",
    "    print(f\"Target shape: {y.shape}\")\n",
    "    print(f\"Buy signals (1): {np.sum(y == 1)}\")\n",
    "    print(f\"No position signals (0): {np.sum(y == 0)}\")\n",
    "    \n",
    "    return X, y, df_processed\n",
    "    \n",
    "stocks_features = {}\n",
    "stocks_targets = {}\n",
    "stocks_processed = {}\n",
    "\n",
    "for stock_name, df in stocks_data.items():\n",
    "    X, y, df_proc = prepare_features_and_target(df, stock_name)\n",
    "    stocks_features[stock_name] = X\n",
    "    stocks_targets[stock_name] = y\n",
    "    stocks_processed[stock_name] = df_proc\n",
    "\n",
    "print(f\"\\nData preparation completed for {len(stocks_features)} stocks\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75359436",
   "metadata": {},
   "source": [
    "## Part 1: Support Vector Classifier for Stock Buying Decisions\n",
    "\n",
    "In this section, we will:\n",
    "1. Implement SVC for each stock to predict buying decisions\n",
    "2. Use TimeSeriesSplit for proper time series cross-validation\n",
    "3. Compare cumulative returns from SVC predictions vs actual returns\n",
    "4. Create visualizations showing the performance\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07985f28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 1: Support Vector Classifier Implementation\n",
    "\n",
    "def train_svc_model(X, y, stock_name, test_size=0.2):\n",
    "    print(f\"\\n=== Training SVC for {stock_name} ===\")\n",
    "    \n",
    "    split_percentage = 1 - test_size\n",
    "    split = int(split_percentage * len(X))\n",
    "    \n",
    "    X_train = X[:split]\n",
    "    y_train = y[:split]\n",
    "    \n",
    "    X_test = X[split:]\n",
    "    y_test = y[split:]\n",
    "    \n",
    "    print(f\"Training set size: {len(X_train)}\")\n",
    "    print(f\"Test set size: {len(X_test)}\")\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    svc = SVC(kernel='rbf', random_state=42)\n",
    "    \n",
    "    svc.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    y_pred_test = svc.predict(X_test_scaled)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"SVC Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Test predictions sample: {y_pred_test[:5]}\")\n",
    "    \n",
    "    return {\n",
    "        'model': svc,\n",
    "        'scaler': scaler,\n",
    "        'y_test': y_test,\n",
    "        'y_pred_test': y_pred_test,\n",
    "        'accuracy': accuracy,\n",
    "        'X_test': X_test,\n",
    "        'X_train': X_train,\n",
    "        'y_train': y_train\n",
    "    }\n",
    "svc_results = {}\n",
    "\n",
    "for stock_name in stocks_features.keys():\n",
    "    X = stocks_features[stock_name]\n",
    "    y = stocks_targets[stock_name]\n",
    "    valid_indices = ~(X.isnull().any(axis=1) | pd.isnull(y))\n",
    "    X_clean = X[valid_indices]\n",
    "    y_clean = y[valid_indices]\n",
    "    \n",
    "    if len(X_clean) > 100: \n",
    "        result = train_svc_model(X_clean, y_clean, stock_name)\n",
    "        svc_results[stock_name] = result\n",
    "    else:\n",
    "        print(f\"Skipping {stock_name} - insufficient data\")\n",
    "\n",
    "print(f\"\\nSVC training completed for {len(svc_results)} stocks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e4b773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Cumulative Returns for SVC Predictions vs Actual Returns\n",
    "\n",
    "def calculate_cumulative_returns(stock_name, svc_result, stock_data):\n",
    "    \n",
    "    test_size = len(svc_result['y_test'])\n",
    "    total_size = len(stock_data)\n",
    "    train_size = total_size - test_size\n",
    "    \n",
    "    test_prices = stock_data['CLOSE'].iloc[train_size:train_size + test_size].values\n",
    "    test_dates = stock_data['Date'].iloc[train_size:train_size + test_size].values\n",
    "    actual_returns = np.diff(test_prices) / test_prices[:-1]\n",
    "    svc_predictions = svc_result['y_pred_test']\n",
    "    svc_returns = np.where(svc_predictions[:-1] == 1, actual_returns, 0)\n",
    "    actual_cumulative = np.cumprod(1 + actual_returns) - 1\n",
    "    svc_cumulative = np.cumprod(1 + svc_returns) - 1\n",
    "    buy_hold_returns = actual_returns\n",
    "    buy_hold_cumulative = np.cumprod(1 + buy_hold_returns) - 1\n",
    "    \n",
    "    return {\n",
    "        'stock_name': stock_name,\n",
    "        'dates': test_dates[1:], \n",
    "        'actual_cumulative': actual_cumulative,\n",
    "        'svc_cumulative': svc_cumulative,\n",
    "        'buy_hold_cumulative': buy_hold_cumulative,\n",
    "        'actual_returns': actual_returns,\n",
    "        'svc_returns': svc_returns,\n",
    "        'svc_predictions': svc_predictions[:-1]  \n",
    "    }\n",
    "\n",
    "cumulative_returns = {}\n",
    "\n",
    "for stock_name in svc_results.keys():\n",
    "    if stock_name in stocks_data:\n",
    "        result = calculate_cumulative_returns(stock_name, svc_results[stock_name], stocks_data[stock_name])\n",
    "        cumulative_returns[stock_name] = result\n",
    "        \n",
    "        # Print summary\n",
    "        final_actual = result['actual_cumulative'][-1]\n",
    "        final_svc = result['svc_cumulative'][-1]\n",
    "        final_buy_hold = result['buy_hold_cumulative'][-1]\n",
    "        \n",
    "        print(f\"\\n{stock_name} - Cumulative Returns:\")\n",
    "        print(f\"Actual Returns: {final_actual:.4f} ({final_actual*100:.2f}%)\")\n",
    "        print(f\"SVC Strategy: {final_svc:.4f} ({final_svc*100:.2f}%)\")\n",
    "        print(f\"Buy & Hold: {final_buy_hold:.4f} ({final_buy_hold*100:.2f}%)\")\n",
    "        print(f\"SVC vs Buy & Hold: {final_svc - final_buy_hold:.4f} ({(final_svc - final_buy_hold)*100:.2f}%)\")\n",
    "\n",
    "print(f\"\\nCumulative returns calculated for {len(cumulative_returns)} stocks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "862019a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_equal_weight_portfolio(cumulative_returns_data):\n",
    "    stock_names = list(cumulative_returns_data.keys())\n",
    "    n_stocks = len(stock_names)\n",
    "    min_length = min(len(data['svc_cumulative']) for data in cumulative_returns_data.values())\n",
    "    portfolio_svc_returns = np.zeros(min_length)\n",
    "    portfolio_buy_hold_returns = np.zeros(min_length)\n",
    "    portfolio_actual_returns = np.zeros(min_length)\n",
    "    for stock_name, data in cumulative_returns_data.items():\n",
    "        svc_returns = data['svc_returns'][:min_length]\n",
    "        buy_hold_returns = data['actual_returns'][:min_length]\n",
    "        actual_returns = data['actual_returns'][:min_length]\n",
    "        portfolio_svc_returns += svc_returns / n_stocks\n",
    "        portfolio_buy_hold_returns += buy_hold_returns / n_stocks\n",
    "        portfolio_actual_returns += actual_returns / n_stocks\n",
    "\n",
    "    portfolio_svc_cumulative = np.cumprod(1 + portfolio_svc_returns) - 1\n",
    "    portfolio_buy_hold_cumulative = np.cumprod(1 + portfolio_buy_hold_returns) - 1\n",
    "    portfolio_actual_cumulative = np.cumprod(1 + portfolio_actual_returns) - 1\n",
    "    first_stock_data = list(cumulative_returns_data.values())[0]\n",
    "    portfolio_dates = first_stock_data['dates'][:min_length]\n",
    "    \n",
    "    return {\n",
    "        'dates': portfolio_dates,\n",
    "        'portfolio_svc_cumulative': portfolio_svc_cumulative,\n",
    "        'portfolio_buy_hold_cumulative': portfolio_buy_hold_cumulative,\n",
    "        'portfolio_actual_cumulative': portfolio_actual_cumulative,\n",
    "        'portfolio_svc_returns': portfolio_svc_returns,\n",
    "        'portfolio_buy_hold_returns': portfolio_buy_hold_returns,\n",
    "        'portfolio_actual_returns': portfolio_actual_returns,\n",
    "        'stock_names': stock_names,\n",
    "        'n_stocks': n_stocks\n",
    "    }\n",
    "\n",
    "if cumulative_returns:\n",
    "    portfolio_data = create_equal_weight_portfolio(cumulative_returns)\n",
    "    final_svc = portfolio_data['portfolio_svc_cumulative'][-1]\n",
    "    final_buy_hold = portfolio_data['portfolio_buy_hold_cumulative'][-1]\n",
    "    final_actual = portfolio_data['portfolio_actual_cumulative'][-1]\n",
    "    \n",
    "    print(f\"\\n=== Equal-Weight Portfolio Performance ===\")\n",
    "    print(f\"Number of stocks: {portfolio_data['n_stocks']}\")\n",
    "    print(f\"Stocks included: {', '.join(portfolio_data['stock_names'])}\")\n",
    "    print(f\"\\nPortfolio Cumulative Returns:\")\n",
    "    print(f\"SVC Strategy: {final_svc:.4f} ({final_svc*100:.2f}%)\")\n",
    "    print(f\"Buy & Hold: {final_buy_hold:.4f} ({final_buy_hold*100:.2f}%)\")\n",
    "    print(f\"Actual Returns: {final_actual:.4f} ({final_actual*100:.2f}%)\")\n",
    "    print(f\"SVC vs Buy & Hold: {final_svc - final_buy_hold:.4f} ({(final_svc - final_buy_hold)*100:.2f}%)\")\n",
    "    \n",
    "    portfolio_volatility = np.std(portfolio_data['portfolio_svc_returns']) * np.sqrt(252)\n",
    "    portfolio_sharpe = (final_svc * 252) / (portfolio_volatility * np.sqrt(252)) if portfolio_volatility > 0 else 0\n",
    "    \n",
    "    print(f\"\\nPortfolio Statistics:\")\n",
    "    print(f\"Annualized Volatility: {portfolio_volatility:.4f} ({portfolio_volatility*100:.2f}%)\")\n",
    "    print(f\"Sharpe Ratio: {portfolio_sharpe:.4f}\")\n",
    "else:\n",
    "    print(\"No cumulative returns data available for portfolio creation\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe36487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Comprehensive Visualizations using Plotly\n",
    "\n",
    "def create_svc_performance_visualization(cumulative_returns_data, portfolio_data):\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=[\n",
    "            'Individual Stock SVC vs Buy & Hold Returns',\n",
    "            'Portfolio Performance Comparison',\n",
    "            'SVC Prediction Accuracy by Stock',\n",
    "            'Cumulative Returns Over Time',\n",
    "            'Return Distribution Comparison',\n",
    "            'Portfolio Risk-Return Analysis'\n",
    "        ],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    stock_names = list(cumulative_returns_data.keys())\n",
    "    svc_final_returns = []\n",
    "    buy_hold_final_returns = []\n",
    "    \n",
    "    for stock_name in stock_names:\n",
    "        data = cumulative_returns_data[stock_name]\n",
    "        svc_final_returns.append(data['svc_cumulative'][-1] * 100)\n",
    "        buy_hold_final_returns.append(data['buy_hold_cumulative'][-1] * 100)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(name='SVC Strategy', x=stock_names, y=svc_final_returns, \n",
    "               marker_color='lightblue', showlegend=False),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(name='Buy & Hold', x=stock_names, y=buy_hold_final_returns,\n",
    "               marker_color='lightcoral', showlegend=False),\n",
    "        row=1, col=1\n",
    "    )\n",
    "    if portfolio_data:\n",
    "        dates = portfolio_data['dates']\n",
    "        portfolio_svc = portfolio_data['portfolio_svc_cumulative'] * 100\n",
    "        portfolio_buy_hold = portfolio_data['portfolio_buy_hold_cumulative'] * 100\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=dates, y=portfolio_svc, mode='lines', name='Portfolio SVC',\n",
    "                      line=dict(color='blue', width=2)),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=dates, y=portfolio_buy_hold, mode='lines', name='Portfolio Buy & Hold',\n",
    "                      line=dict(color='red', width=2)),\n",
    "            row=1, col=2\n",
    "        )\n",
    "\n",
    "    accuracies = []\n",
    "    for stock_name in stock_names:\n",
    "        if stock_name in svc_results:\n",
    "            accuracies.append(svc_results[stock_name]['accuracy'] * 100)\n",
    "        else:\n",
    "            accuracies.append(0)\n",
    "    \n",
    "    fig.add_trace(\n",
    "        go.Bar(x=stock_names, y=accuracies, marker_color='green', showlegend=False),\n",
    "        row=2, col=1\n",
    "    )\n",
    "    colors = ['blue', 'red', 'green', 'orange', 'purple']\n",
    "    for i, (stock_name, data) in enumerate(list(cumulative_returns_data.items())[:5]):\n",
    "        if i < len(colors):\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=data['dates'], y=data['svc_cumulative'] * 100,\n",
    "                          mode='lines', name=f'{stock_name} SVC',\n",
    "                          line=dict(color=colors[i], width=1)),\n",
    "                row=2, col=2\n",
    "            )\n",
    "    if portfolio_data:\n",
    "        svc_returns = portfolio_data['portfolio_svc_returns']\n",
    "        buy_hold_returns = portfolio_data['portfolio_buy_hold_returns']\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=svc_returns, name='SVC Returns', opacity=0.7,\n",
    "                        marker_color='blue', showlegend=False),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Histogram(x=buy_hold_returns, name='Buy & Hold Returns', opacity=0.7,\n",
    "                        marker_color='red', showlegend=False),\n",
    "            row=3, col=1\n",
    "        )\n",
    "    \n",
    "\n",
    "    if portfolio_data:\n",
    "        risks = []\n",
    "        returns = []\n",
    "        \n",
    "        for stock_name, data in cumulative_returns_data.items():\n",
    "            stock_returns = data['svc_returns']\n",
    "            risk = np.std(stock_returns) * np.sqrt(252) * 100  # Annualized volatility\n",
    "            return_val = data['svc_cumulative'][-1] * 100  # Total return\n",
    "            \n",
    "            risks.append(risk)\n",
    "            returns.append(return_val)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=risks, y=returns, mode='markers+text',\n",
    "                      text=stock_names, textposition=\"top center\",\n",
    "                      marker=dict(size=10, color='blue'),\n",
    "                      showlegend=False),\n",
    "            row=3, col=2\n",
    "        )\n",
    "    fig.update_layout(\n",
    "        title_text=\"SVC Stock Market Prediction Performance Analysis\",\n",
    "        title_x=0.5,\n",
    "        height=1200,\n",
    "        showlegend=True\n",
    "    )\n",
    "    fig.update_xaxes(title_text=\"Stocks\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"Returns (%)\", row=1, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Date\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Cumulative Returns (%)\", row=1, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Stocks\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Accuracy (%)\", row=2, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Date\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"Cumulative Returns (%)\", row=2, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Daily Returns\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Frequency\", row=3, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Risk (Volatility %)\", row=3, col=2)\n",
    "    fig.update_yaxes(title_text=\"Return (%)\", row=3, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "if cumulative_returns and 'portfolio_data' in locals():\n",
    "    create_svc_performance_visualization(cumulative_returns, portfolio_data)\n",
    "else:\n",
    "    print(\"Insufficient data for visualization\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3009c13",
   "metadata": {},
   "source": [
    "## Part 2: ARIMA, SVR, XGBoost, and Neural Networks for Stock Price Prediction\n",
    "\n",
    "In this section, we will:\n",
    "1. Implement SARIMAX for time series forecasting\n",
    "2. Use Support Vector Regression (SVR) for price prediction\n",
    "3. Apply XGBoost for enhanced prediction accuracy\n",
    "4. Build Neural Networks (LSTM/GRU) for deep learning approach\n",
    "5. Compare all models using MSE, MAE, and R² metrics\n",
    "6. Create comprehensive visualizations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3045827d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: SARIMAX Implementation for Stock Price Prediction\n",
    "\n",
    "def prepare_sarimax_data(df, stock_name, target_col='CLOSE'):\n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    df_processed.index = pd.to_datetime(df_processed['Date'])\n",
    "    \n",
    "    df_processed['Open-Close'] = df_processed['OPEN'] - df_processed['CLOSE']\n",
    "    df_processed['High-Low'] = df_processed['HIGH'] - df_processed['LOW']\n",
    "    df_processed['Volume'] = df_processed.get('VOLUME', 1)  # Use volume if available, else 1\n",
    "\n",
    "    exog_vars = ['Open-Close', 'High-Low', 'Volume']\n",
    "    exog_data = df_processed[exog_vars].fillna(method='ffill').fillna(0)\n",
    "    \n",
    "    target_data = df_processed[target_col].dropna()\n",
    "    \n",
    "    common_index = target_data.index.intersection(exog_data.index)\n",
    "    target_data = target_data.loc[common_index]\n",
    "    exog_data = exog_data.loc[common_index]\n",
    "    split_point = int(0.8 * len(target_data))\n",
    "    \n",
    "    train_target = target_data[:split_point]\n",
    "    test_target = target_data[split_point:]\n",
    "    train_exog = exog_data[:split_point]\n",
    "    test_exog = exog_data[split_point:]\n",
    "    \n",
    "    print(f\"{stock_name} - SARIMAX Data Preparation:\")\n",
    "    print(f\"Training period: {train_target.index[0]} to {train_target.index[-1]}\")\n",
    "    print(f\"Testing period: {test_target.index[0]} to {test_target.index[-1]}\")\n",
    "    print(f\"Training samples: {len(train_target)}\")\n",
    "    print(f\"Testing samples: {len(test_target)}\")\n",
    "    \n",
    "    return train_target, test_target, train_exog, test_exog\n",
    "\n",
    "def fit_sarimax_model(train_target, train_exog, stock_name):\n",
    "    try:\n",
    "        model = SARIMAX(\n",
    "            train_target,\n",
    "            exog=train_exog,\n",
    "            order=(1, 1, 1),  # (p, d, q)\n",
    "            seasonal_order=(1, 1, 1, 12),  # (P, D, Q, s) - monthly seasonality\n",
    "            enforce_stationarity=False,\n",
    "            enforce_invertibility=False\n",
    "        )\n",
    "        \n",
    "        fitted_model = model.fit(disp=False, maxiter=50)\n",
    "        \n",
    "        print(f\"{stock_name} - SARIMAX Model fitted successfully\")\n",
    "        print(f\"AIC: {fitted_model.aic:.2f}\")\n",
    "        print(f\"BIC: {fitted_model.bic:.2f}\")\n",
    "        \n",
    "        return fitted_model\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{stock_name} - SARIMAX Model failed: {str(e)}\")\n",
    "        try:\n",
    "            model = SARIMAX(\n",
    "                train_target,\n",
    "                order=(1, 1, 1),\n",
    "                enforce_stationarity=False,\n",
    "                enforce_invertibility=False\n",
    "            )\n",
    "            fitted_model = model.fit(disp=False, maxiter=50)\n",
    "            print(f\"{stock_name} - Simple SARIMAX Model fitted successfully\")\n",
    "            return fitted_model\n",
    "        except Exception as e2:\n",
    "            print(f\"{stock_name} - Simple SARIMAX also failed: {str(e2)}\")\n",
    "            return None\n",
    "\n",
    "def predict_sarimax(model, test_exog, steps):\n",
    "    try:\n",
    "        if model is None:\n",
    "            return np.full(steps, np.nan)\n",
    "        \n",
    "        predictions = model.forecast(steps=steps, exog=test_exog)\n",
    "        return predictions.values if hasattr(predictions, 'values') else predictions\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"SARIMAX prediction error: {str(e)}\")\n",
    "        return np.full(steps, np.nan)\n",
    "\n",
    "sarimax_results = {}\n",
    "\n",
    "for stock_name, df in stocks_data.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {stock_name} for SARIMAX\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    train_target, test_target, train_exog, test_exog = prepare_sarimax_data(df, stock_name)\n",
    "    \n",
    "    model = fit_sarimax_model(train_target, train_exog, stock_name)\n",
    "    \n",
    "    if model is not None:\n",
    "        predictions = predict_sarimax(model, test_exog, len(test_target))\n",
    "        \n",
    "        mse = mean_squared_error(test_target, predictions)\n",
    "        mae = mean_absolute_error(test_target, predictions)\n",
    "        r2 = r2_score(test_target, predictions)\n",
    "        \n",
    "        sarimax_results[stock_name] = {\n",
    "            'model': model,\n",
    "            'predictions': predictions,\n",
    "            'actual': test_target.values,\n",
    "            'dates': test_target.index,\n",
    "            'mse': mse,\n",
    "            'mae': mae,\n",
    "            'r2': r2,\n",
    "            'train_target': train_target,\n",
    "            'test_target': test_target\n",
    "        }\n",
    "        \n",
    "        print(f\"{stock_name} - SARIMAX Performance:\")\n",
    "        print(f\"MSE: {mse:.4f}\")\n",
    "        print(f\"MAE: {mae:.4f}\")\n",
    "        print(f\"R²: {r2:.4f}\")\n",
    "    else:\n",
    "        print(f\"{stock_name} - SARIMAX model could not be fitted\")\n",
    "\n",
    "print(f\"\\nSARIMAX models completed for {len(sarimax_results)} stocks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b51e1909",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Support Vector Regression (SVR) Implementation\n",
    "\n",
    "def prepare_svr_data(df, stock_name, target_col='CLOSE'):\n",
    "    df_processed = df.copy()\n",
    "    df_processed['Open-Close'] = df_processed['OPEN'] - df_processed['CLOSE']\n",
    "    df_processed['High-Low'] = df_processed['HIGH'] - df_processed['LOW']\n",
    "    df_processed['Price_Change'] = df_processed['CLOSE'].pct_change()\n",
    "    df_processed['SMA_5'] = df_processed['CLOSE'].rolling(window=5).mean()\n",
    "    df_processed['SMA_10'] = df_processed['CLOSE'].rolling(window=10).mean()\n",
    "    df_processed['Volatility'] = df_processed['Price_Change'].rolling(window=10).std()\n",
    "    \n",
    "    feature_cols = ['Open-Close', 'High-Low', 'Price_Change', 'SMA_5', 'SMA_10', 'Volatility']\n",
    "    X = df_processed[feature_cols].fillna(method='ffill').fillna(0)\n",
    "    y = df_processed[target_col]\n",
    "    \n",
    "    valid_indices = ~(X.isnull().any(axis=1) | y.isnull())\n",
    "    X = X[valid_indices]\n",
    "    y = y[valid_indices]\n",
    "    \n",
    "    split_point = int(0.8 * len(X))\n",
    "    \n",
    "    X_train = X[:split_point]\n",
    "    X_test = X[split_point:]\n",
    "    y_train = y[:split_point]\n",
    "    y_test = y[split_point:]\n",
    "    \n",
    "    print(f\"{stock_name} - SVR Data Preparation:\")\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Testing samples: {len(X_test)}\")\n",
    "    print(f\"Features: {list(X.columns)}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, X.index[split_point:]\n",
    "\n",
    "def train_svr_model(X_train, X_test, y_train, y_test, stock_name):\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    svr = SVR(kernel='rbf', C=1.0, gamma='scale', epsilon=0.1)\n",
    "    \n",
    "    svr.fit(X_train_scaled, y_train)\n",
    "    y_pred_train = svr.predict(X_train_scaled)\n",
    "    y_pred_test = svr.predict(X_test_scaled)\n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"{stock_name} - SVR Performance:\")\n",
    "    print(f\"Training MSE: {train_mse:.4f}, MAE: {train_mae:.4f}, R²: {train_r2:.4f}\")\n",
    "    print(f\"Testing MSE: {test_mse:.4f}, MAE: {test_mae:.4f}, R²: {test_r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': svr,\n",
    "        'scaler': scaler,\n",
    "        'predictions': y_pred_test,\n",
    "        'actual': y_test.values,\n",
    "        'mse': test_mse,\n",
    "        'mae': test_mae,\n",
    "        'r2': test_r2,\n",
    "        'train_predictions': y_pred_train,\n",
    "        'train_actual': y_train.values\n",
    "    }\n",
    "\n",
    "svr_results = {}\n",
    "\n",
    "for stock_name, df in stocks_data.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {stock_name} for SVR\")\n",
    "    print(f\"{'='*50}\")\n",
    "    X_train, X_test, y_train, y_test, test_dates = prepare_svr_data(df, stock_name)\n",
    "    result = train_svr_model(X_train, X_test, y_train, y_test, stock_name)\n",
    "    result['dates'] = test_dates\n",
    "    \n",
    "    svr_results[stock_name] = result\n",
    "\n",
    "print(f\"\\nSVR models completed for {len(svr_results)} stocks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1e4cbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: XGBoost Implementation\n",
    "\n",
    "def train_xgboost_model(X_train, X_test, y_train, y_test, stock_name):\n",
    "    xgb_model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=6,\n",
    "        learning_rate=0.1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    xgb_model.fit(X_train, y_train)\n",
    "    y_pred_train = xgb_model.predict(X_train)\n",
    "    y_pred_test = xgb_model.predict(X_test)\n",
    "    \n",
    "    train_mse = mean_squared_error(y_train, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test, y_pred_test)\n",
    "    train_mae = mean_absolute_error(y_train, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test, y_pred_test)\n",
    "    train_r2 = r2_score(y_train, y_pred_train)\n",
    "    test_r2 = r2_score(y_test, y_pred_test)\n",
    "    \n",
    "    print(f\"{stock_name} - XGBoost Performance:\")\n",
    "    print(f\"Training MSE: {train_mse:.4f}, MAE: {train_mae:.4f}, R²: {train_r2:.4f}\")\n",
    "    print(f\"Testing MSE: {test_mse:.4f}, MAE: {test_mae:.4f}, R²: {test_r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': xgb_model,\n",
    "        'predictions': y_pred_test,\n",
    "        'actual': y_test.values,\n",
    "        'mse': test_mse,\n",
    "        'mae': test_mae,\n",
    "        'r2': test_r2,\n",
    "        'train_predictions': y_pred_train,\n",
    "        'train_actual': y_train.values\n",
    "    }\n",
    "xgboost_results = {}\n",
    "\n",
    "for stock_name, df in stocks_data.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {stock_name} for XGBoost\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test, test_dates = prepare_svr_data(df, stock_name)\n",
    "    \n",
    "    result = train_xgboost_model(X_train, X_test, y_train, y_test, stock_name)\n",
    "    result['dates'] = test_dates\n",
    "    \n",
    "    xgboost_results[stock_name] = result\n",
    "\n",
    "print(f\"\\nXGBoost models completed for {len(xgboost_results)} stocks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f05696",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 2: Neural Network Implementation (LSTM/GRU)\n",
    "\n",
    "def prepare_lstm_data(df, stock_name, target_col='CLOSE', sequence_length=60):\n",
    "    df_processed = df.copy()\n",
    "    df_processed['Open-Close'] = df_processed['OPEN'] - df_processed['CLOSE']\n",
    "    df_processed['High-Low'] = df_processed['HIGH'] - df_processed['LOW']\n",
    "    df_processed['Price_Change'] = df_processed['CLOSE'].pct_change()\n",
    "    df_processed['SMA_5'] = df_processed['CLOSE'].rolling(window=5).mean()\n",
    "    df_processed['SMA_10'] = df_processed['CLOSE'].rolling(window=10).mean()\n",
    "    df_processed['Volatility'] = df_processed['Price_Change'].rolling(window=10).std()\n",
    "    \n",
    "    feature_cols = ['Open-Close', 'High-Low', 'Price_Change', 'SMA_5', 'SMA_10', 'Volatility']\n",
    "    features = df_processed[feature_cols].fillna(method='ffill').fillna(0).values\n",
    "    target = df_processed[target_col].fillna(method='ffill').values\n",
    "    \n",
    "    scaler_features = MinMaxScaler()\n",
    "    scaler_target = MinMaxScaler()\n",
    "    \n",
    "    features_scaled = scaler_features.fit_transform(features)\n",
    "    target_scaled = scaler_target.fit_transform(target.reshape(-1, 1)).flatten()\n",
    "    \n",
    "    X, y = [], []\n",
    "    for i in range(sequence_length, len(features_scaled)):\n",
    "        X.append(features_scaled[i-sequence_length:i])\n",
    "        y.append(target_scaled[i])\n",
    "    \n",
    "    X, y = np.array(X), np.array(y)\n",
    "    \n",
    "    split_point = int(0.8 * len(X))\n",
    "    \n",
    "    X_train = X[:split_point]\n",
    "    X_test = X[split_point:]\n",
    "    y_train = y[:split_point]\n",
    "    y_test = y[split_point:]\n",
    "    \n",
    "    print(f\"{stock_name} - LSTM Data Preparation:\")\n",
    "    print(f\"Sequence length: {sequence_length}\")\n",
    "    print(f\"Training samples: {len(X_train)}\")\n",
    "    print(f\"Testing samples: {len(X_test)}\")\n",
    "    print(f\"Feature shape: {X_train.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler_features, scaler_target\n",
    "\n",
    "def create_lstm_model(input_shape, stock_name):\n",
    "    model = Sequential([\n",
    "        LSTM(50, return_sequences=True, input_shape=input_shape),\n",
    "        Dropout(0.2),\n",
    "        LSTM(50, return_sequences=False),\n",
    "        Dropout(0.2),\n",
    "        Dense(25),\n",
    "        Dense(1)\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "    \n",
    "    print(f\"{stock_name} - LSTM Model created:\")\n",
    "    print(f\"Input shape: {input_shape}\")\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def train_lstm_model(X_train, X_test, y_train, y_test, scaler_target, stock_name):\n",
    "    model = create_lstm_model((X_train.shape[1], X_train.shape[2]), stock_name)\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=0.0001)\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        batch_size=32,\n",
    "        epochs=50,\n",
    "        validation_data=(X_test, y_test),\n",
    "        callbacks=[early_stopping, reduce_lr],\n",
    "        verbose=0\n",
    "    )\n",
    "    y_pred_train = model.predict(X_train, verbose=0)\n",
    "    y_pred_test = model.predict(X_test, verbose=0)\n",
    "    y_pred_train = scaler_target.inverse_transform(y_pred_train).flatten()\n",
    "    y_pred_test = scaler_target.inverse_transform(y_pred_test).flatten()\n",
    "    y_train_actual = scaler_target.inverse_transform(y_train.reshape(-1, 1)).flatten()\n",
    "    y_test_actual = scaler_target.inverse_transform(y_test.reshape(-1, 1)).flatten()\n",
    "    train_mse = mean_squared_error(y_train_actual, y_pred_train)\n",
    "    test_mse = mean_squared_error(y_test_actual, y_pred_test)\n",
    "    train_mae = mean_absolute_error(y_train_actual, y_pred_train)\n",
    "    test_mae = mean_absolute_error(y_test_actual, y_pred_test)\n",
    "    train_r2 = r2_score(y_train_actual, y_pred_train)\n",
    "    test_r2 = r2_score(y_test_actual, y_pred_test)\n",
    "    \n",
    "    print(f\"{stock_name} - LSTM Performance:\")\n",
    "    print(f\"Training MSE: {train_mse:.4f}, MAE: {train_mae:.4f}, R²: {train_r2:.4f}\")\n",
    "    print(f\"Testing MSE: {test_mse:.4f}, MAE: {test_mae:.4f}, R²: {test_r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'model': model,\n",
    "        'history': history,\n",
    "        'predictions': y_pred_test,\n",
    "        'actual': y_test_actual,\n",
    "        'mse': test_mse,\n",
    "        'mae': test_mae,\n",
    "        'r2': test_r2,\n",
    "        'train_predictions': y_pred_train,\n",
    "        'train_actual': y_train_actual\n",
    "    }\n",
    "lstm_results = {}\n",
    "\n",
    "for stock_name, df in stocks_data.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Processing {stock_name} for LSTM\")\n",
    "    print(f\"{'='*50}\")\n",
    "    \n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test, scaler_features, scaler_target = prepare_lstm_data(df, stock_name)\n",
    "        result = train_lstm_model(X_train, X_test, y_train, y_test, scaler_target, stock_name)\n",
    "        \n",
    "        lstm_results[stock_name] = result\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"{stock_name} - LSTM training failed: {str(e)}\")\n",
    "\n",
    "print(f\"\\nLSTM models completed for {len(lstm_results)} stocks\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce316803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Performance Comparison and Analysis\n",
    "\n",
    "def compare_model_performance():\n",
    "    print(\"=\"*80)\n",
    "    print(\"MODEL PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Collect all results\n",
    "    all_results = {\n",
    "        'SARIMAX': sarimax_results,\n",
    "        'SVR': svr_results,\n",
    "        'XGBoost': xgboost_results,\n",
    "        'LSTM': lstm_results\n",
    "    }\n",
    "    \n",
    "    comparison_data = []\n",
    "    \n",
    "    for model_name, results in all_results.items():\n",
    "        for stock_name, result in results.items():\n",
    "            comparison_data.append({\n",
    "                'Model': model_name,\n",
    "                'Stock': stock_name,\n",
    "                'MSE': result['mse'],\n",
    "                'MAE': result['mae'],\n",
    "                'R²': result['r2']\n",
    "            })\n",
    "    \n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    \n",
    "    print(\"\\nSUMMARY STATISTICS BY MODEL:\")\n",
    "    print(\"-\" * 50)\n",
    "    summary_stats = comparison_df.groupby('Model').agg({\n",
    "        'MSE': ['mean', 'std'],\n",
    "        'MAE': ['mean', 'std'],\n",
    "        'R²': ['mean', 'std']\n",
    "    }).round(4)\n",
    "    \n",
    "    print(summary_stats)\n",
    "    print(\"\\nBEST MODEL FOR EACH STOCK (by R²):\")\n",
    "    print(\"-\" * 50)\n",
    "    best_models = comparison_df.loc[comparison_df.groupby('Stock')['R²'].idxmax()]\n",
    "    for _, row in best_models.iterrows():\n",
    "        print(f\"{row['Stock']}: {row['Model']} (R² = {row['R²']:.4f})\")\n",
    "    print(\"\\nOVERALL BEST MODEL (by average R²):\")\n",
    "    print(\"-\" * 50)\n",
    "    avg_r2 = comparison_df.groupby('Model')['R²'].mean().sort_values(ascending=False)\n",
    "    for model, r2 in avg_r2.items():\n",
    "        print(f\"{model}: {r2:.4f}\")\n",
    "    \n",
    "    return comparison_df\n",
    "if 'sarimax_results' in locals() and 'svr_results' in locals() and 'xgboost_results' in locals() and 'lstm_results' in locals():\n",
    "    comparison_df = compare_model_performance()\n",
    "else:\n",
    "    print(\"Some models are not yet trained. Please run all model training cells first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414b9e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive Visualization of All Models\n",
    "\n",
    "def create_comprehensive_visualization():\n",
    "    \"\"\"\n",
    "    Create comprehensive Plotly visualizations for all models\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create subplots\n",
    "    fig = make_subplots(\n",
    "        rows=4, cols=2,\n",
    "        subplot_titles=[\n",
    "            'Model Performance Comparison (MSE)',\n",
    "            'Model Performance Comparison (R²)',\n",
    "            'Actual vs Predicted - SARIMAX',\n",
    "            'Actual vs Predicted - SVR',\n",
    "            'Actual vs Predicted - XGBoost',\n",
    "            'Actual vs Predicted - LSTM',\n",
    "            'Model Accuracy by Stock',\n",
    "            'Training Loss Curves (LSTM)'\n",
    "        ],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "    \n",
    "    if 'comparison_df' in locals():\n",
    "        mse_data = comparison_df.groupby('Model')['MSE'].mean().sort_values()\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=mse_data.index, y=mse_data.values, name='Average MSE',\n",
    "                   marker_color='lightcoral', showlegend=False),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    if 'comparison_df' in locals():\n",
    "        r2_data = comparison_df.groupby('Model')['R²'].mean().sort_values(ascending=False)\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=r2_data.index, y=r2_data.values, name='Average R²',\n",
    "                   marker_color='lightgreen', showlegend=False),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    models_data = {\n",
    "        'SARIMAX': sarimax_results,\n",
    "        'SVR': svr_results,\n",
    "        'XGBoost': xgboost_results,\n",
    "        'LSTM': lstm_results\n",
    "    }\n",
    "    \n",
    "    positions = [(2, 1), (2, 2), (3, 1), (3, 2)]\n",
    "    \n",
    "    for i, (model_name, results) in enumerate(models_data.items()):\n",
    "        if results and i < len(positions):\n",
    "            row, col = positions[i]\n",
    "            stock_name = list(results.keys())[0]\n",
    "            result = results[stock_name]\n",
    "            \n",
    "            actual = result['actual']\n",
    "            predicted = result['predictions']\n",
    "            \n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=actual, y=predicted, mode='markers',\n",
    "                          name=f'{model_name} - {stock_name}',\n",
    "                          marker=dict(size=6, opacity=0.6),\n",
    "                          showlegend=False),\n",
    "                row=row, col=col\n",
    "            )\n",
    "            min_val = min(min(actual), min(predicted))\n",
    "            max_val = max(max(actual), max(predicted))\n",
    "            fig.add_trace(\n",
    "                go.Scatter(x=[min_val, max_val], y=[min_val, max_val],\n",
    "                          mode='lines', name='Perfect Prediction',\n",
    "                          line=dict(dash='dash', color='red'),\n",
    "                          showlegend=False),\n",
    "                row=row, col=col\n",
    "            )\n",
    "    \n",
    "    if 'comparison_df' in locals():\n",
    "        pivot_data = comparison_df.pivot(index='Stock', columns='Model', values='R²')\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Heatmap(z=pivot_data.values,\n",
    "                      x=pivot_data.columns,\n",
    "                      y=pivot_data.index,\n",
    "                      colorscale='RdYlGn',\n",
    "                      showscale=True,\n",
    "                      name='R² Score'),\n",
    "            row=4, col=1\n",
    "        )\n",
    "    \n",
    "    if lstm_results:\n",
    "        stock_name = list(lstm_results.keys())[0]\n",
    "        history = lstm_results[stock_name]['history']\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=history.history['loss'], mode='lines',\n",
    "                      name='Training Loss', line=dict(color='blue')),\n",
    "            row=4, col=2\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(y=history.history['val_loss'], mode='lines',\n",
    "                      name='Validation Loss', line=dict(color='red')),\n",
    "            row=4, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=\"Comprehensive Stock Price Prediction Analysis\",\n",
    "        title_x=0.5,\n",
    "        height=1600,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Models\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"MSE\", row=1, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Models\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"R²\", row=1, col=2)\n",
    "    \n",
    "    for row in [2, 3]:\n",
    "        for col in [1, 2]:\n",
    "            fig.update_xaxes(title_text=\"Actual Price\", row=row, col=col)\n",
    "            fig.update_yaxes(title_text=\"Predicted Price\", row=row, col=col)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Models\", row=4, col=1)\n",
    "    fig.update_yaxes(title_text=\"Stocks\", row=4, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Epochs\", row=4, col=2)\n",
    "    fig.update_yaxes(title_text=\"Loss\", row=4, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "create_comprehensive_visualization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50f15cf",
   "metadata": {},
   "source": [
    "## Advanced Model: Hybrid ARIMA-ANN Approach\n",
    "\n",
    "In this section, we will implement a hybrid approach combining ARIMA with Artificial Neural Networks (ANN) as mentioned in the project requirements. This approach leverages the strengths of both time series analysis and deep learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed036ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_hybrid_arima_ann_model(df, stock_name, target_col='CLOSE'):\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Creating Hybrid ARIMA-ANN Model for {stock_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    \n",
    "    df_processed = df.copy()\n",
    "    df_processed.index = pd.to_datetime(df_processed['Date'])\n",
    "    \n",
    "    \n",
    "    df_processed['Open-Close'] = df_processed['OPEN'] - df_processed['CLOSE']\n",
    "    df_processed['High-Low'] = df_processed['HIGH'] - df_processed['LOW']\n",
    "    df_processed['Price_Change'] = df_processed['CLOSE'].pct_change()\n",
    "    df_processed['SMA_5'] = df_processed['CLOSE'].rolling(window=5).mean()\n",
    "    df_processed['SMA_10'] = df_processed['CLOSE'].rolling(window=10).mean()\n",
    "    df_processed['Volatility'] = df_processed['Price_Change'].rolling(window=10).std()\n",
    "    \n",
    "    target_data = df_processed[target_col].dropna()\n",
    "    \n",
    "    split_point = int(0.8 * len(target_data))\n",
    "    train_target = target_data[:split_point]\n",
    "    test_target = target_data[split_point:]\n",
    "    \n",
    "    try:\n",
    "        arima_model = ARIMA(train_target, order=(1, 1, 1))\n",
    "        arima_fitted = arima_model.fit()\n",
    "        \n",
    "        arima_pred_train = arima_fitted.fittedvalues\n",
    "        arima_pred_test = arima_fitted.forecast(steps=len(test_target))\n",
    "        \n",
    "        print(f\"ARIMA model fitted successfully\")\n",
    "        print(f\"ARIMA AIC: {arima_fitted.aic:.2f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"ARIMA model failed: {str(e)}\")\n",
    "        arima_pred_train = train_target.rolling(window=5).mean().fillna(train_target.mean())\n",
    "        arima_pred_test = np.full(len(test_target), train_target.mean())\n",
    "    \n",
    "    arima_residuals_train = train_target - arima_pred_train\n",
    "    arima_residuals_test = test_target - arima_pred_test\n",
    "    \n",
    "    feature_cols = ['Open-Close', 'High-Low', 'Price_Change', 'SMA_5', 'SMA_10', 'Volatility']\n",
    "    features = df_processed[feature_cols].fillna(method='ffill').fillna(0)\n",
    "\n",
    "    common_index = target_data.index.intersection(features.index)\n",
    "    features_aligned = features.loc[common_index]\n",
    "    target_aligned = target_data.loc[common_index]\n",
    "    \n",
    "    train_features = features_aligned[:split_point]\n",
    "    test_features = features_aligned[split_point:]\n",
    "    \n",
    "    def create_ann_model(input_dim):\n",
    "        model = Sequential([\n",
    "            Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "            Dropout(0.3),\n",
    "            Dense(32, activation='relu'),\n",
    "            Dropout(0.3),\n",
    "            Dense(16, activation='relu'),\n",
    "            Dense(1)\n",
    "        ])\n",
    "        \n",
    "        model.compile(optimizer=Adam(learning_rate=0.001), loss='mse', metrics=['mae'])\n",
    "        return model\n",
    "    \n",
    "    scaler_features = StandardScaler()\n",
    "    train_features_scaled = scaler_features.fit_transform(train_features)\n",
    "    test_features_scaled = scaler_features.transform(test_features)\n",
    "    \n",
    "    ann_model = create_ann_model(train_features_scaled.shape[1])\n",
    "    \n",
    "    ann_model.fit(\n",
    "        train_features_scaled, arima_residuals_train,\n",
    "        epochs=50, batch_size=32, verbose=0,\n",
    "        validation_split=0.2\n",
    "    )\n",
    "    \n",
    "    ann_residuals_train = ann_model.predict(train_features_scaled, verbose=0).flatten()\n",
    "    ann_residuals_test = ann_model.predict(test_features_scaled, verbose=0).flatten()\n",
    "    hybrid_pred_train = arima_pred_train + ann_residuals_train\n",
    "    hybrid_pred_test = arima_pred_test + ann_residuals_test\n",
    "    train_mse = mean_squared_error(train_target, hybrid_pred_train)\n",
    "    test_mse = mean_squared_error(test_target, hybrid_pred_test)\n",
    "    train_mae = mean_absolute_error(train_target, hybrid_pred_train)\n",
    "    test_mae = mean_absolute_error(test_target, hybrid_pred_test)\n",
    "    train_r2 = r2_score(train_target, hybrid_pred_train)\n",
    "    test_r2 = r2_score(test_target, hybrid_pred_test)\n",
    "    \n",
    "    print(f\"\\nHybrid ARIMA-ANN Performance for {stock_name}:\")\n",
    "    print(f\"Training MSE: {train_mse:.4f}, MAE: {train_mae:.4f}, R²: {train_r2:.4f}\")\n",
    "    print(f\"Testing MSE: {test_mse:.4f}, MAE: {test_mae:.4f}, R²: {test_r2:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'arima_model': arima_fitted if 'arima_fitted' in locals() else None,\n",
    "        'ann_model': ann_model,\n",
    "        'predictions': hybrid_pred_test,\n",
    "        'actual': test_target.values,\n",
    "        'mse': test_mse,\n",
    "        'mae': test_mae,\n",
    "        'r2': test_r2,\n",
    "        'train_predictions': hybrid_pred_train,\n",
    "        'train_actual': train_target.values,\n",
    "        'arima_predictions': arima_pred_test,\n",
    "        'ann_residuals': ann_residuals_test\n",
    "    }\n",
    "\n",
    "\n",
    "if stocks_data:\n",
    "    stock_name = list(stocks_data.keys())[0]\n",
    "    df = stocks_data[stock_name]\n",
    "    \n",
    "    hybrid_results = create_hybrid_arima_ann_model(df, stock_name)\n",
    "    \n",
    "    print(f\"\\nHybrid ARIMA-ANN model completed for {stock_name}\")\n",
    "else:\n",
    "    print(\"No stock data available for hybrid model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb187bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final Model Comparison and Visualization\n",
    "\n",
    "def create_final_comparison_visualization():\n",
    "    fig = make_subplots(\n",
    "        rows=3, cols=2,\n",
    "        subplot_titles=[\n",
    "            'Model Performance Comparison (All Models)',\n",
    "            'Actual vs Predicted - Hybrid ARIMA-ANN',\n",
    "            'Model Accuracy Heatmap',\n",
    "            'Prediction Error Distribution',\n",
    "            'Cumulative Returns Comparison',\n",
    "            'Model Complexity vs Performance'\n",
    "        ],\n",
    "        specs=[[{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}],\n",
    "               [{\"secondary_y\": False}, {\"secondary_y\": False}]]\n",
    "    )\n",
    "\n",
    "    if 'comparison_df' in locals() and 'hybrid_results' in locals():\n",
    "        \n",
    "        hybrid_data = {\n",
    "            'Model': 'Hybrid ARIMA-ANN',\n",
    "            'Stock': list(stocks_data.keys())[0],\n",
    "            'MSE': hybrid_results['mse'],\n",
    "            'MAE': hybrid_results['mae'],\n",
    "            'R²': hybrid_results['r2']\n",
    "        }\n",
    "        \n",
    "        final_comparison = pd.concat([comparison_df, pd.DataFrame([hybrid_data])], ignore_index=True)\n",
    "        r2_data = final_comparison.groupby('Model')['R²'].mean().sort_values(ascending=False)\n",
    "        fig.add_trace(\n",
    "            go.Bar(x=r2_data.index, y=r2_data.values, name='Average R²',\n",
    "                   marker_color='lightblue', showlegend=False),\n",
    "            row=1, col=1\n",
    "        )\n",
    "    \n",
    "    if 'hybrid_results' in locals():\n",
    "        actual = hybrid_results['actual']\n",
    "        predicted = hybrid_results['predictions']\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=actual, y=predicted, mode='markers',\n",
    "                      name='Hybrid Predictions',\n",
    "                      marker=dict(size=8, color='purple', opacity=0.7),\n",
    "                      showlegend=False),\n",
    "            row=1, col=2\n",
    "        )\n",
    "        \n",
    "        # Add perfect prediction line\n",
    "        min_val = min(min(actual), min(predicted))\n",
    "        max_val = max(max(actual), max(predicted))\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=[min_val, max_val], y=[min_val, max_val],\n",
    "                      mode='lines', name='Perfect Prediction',\n",
    "                      line=dict(dash='dash', color='red'),\n",
    "                      showlegend=False),\n",
    "            row=1, col=2\n",
    "        )\n",
    "    \n",
    "    if 'final_comparison' in locals():\n",
    "        pivot_data = final_comparison.pivot(index='Stock', columns='Model', values='R²')\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Heatmap(z=pivot_data.values,\n",
    "                      x=pivot_data.columns,\n",
    "                      y=pivot_data.index,\n",
    "                      colorscale='RdYlGn',\n",
    "                      showscale=True,\n",
    "                      name='R² Score'),\n",
    "            row=2, col=1\n",
    "        )\n",
    "    \n",
    "    if 'final_comparison' in locals():\n",
    "        error_data = []\n",
    "        for model in final_comparison['Model'].unique():\n",
    "            model_data = final_comparison[final_comparison['Model'] == model]\n",
    "            avg_mse = model_data['MSE'].mean()\n",
    "            error_data.append({'Model': model, 'MSE': avg_mse})\n",
    "        \n",
    "        error_df = pd.DataFrame(error_data)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Bar(x=error_df['Model'], y=error_df['MSE'],\n",
    "                   name='Average MSE', marker_color='lightcoral',\n",
    "                   showlegend=False),\n",
    "            row=2, col=2\n",
    "        )\n",
    "    if 'cumulative_returns' in locals() and 'portfolio_data' in locals():\n",
    "        dates = portfolio_data['dates']\n",
    "        portfolio_svc = portfolio_data['portfolio_svc_cumulative'] * 100\n",
    "        portfolio_buy_hold = portfolio_data['portfolio_buy_hold_cumulative'] * 100\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=dates, y=portfolio_svc, mode='lines',\n",
    "                      name='SVC Portfolio', line=dict(color='blue', width=2)),\n",
    "            row=3, col=1\n",
    "        )\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=dates, y=portfolio_buy_hold, mode='lines',\n",
    "                      name='Buy & Hold Portfolio', line=dict(color='red', width=2)),\n",
    "            row=3, col=1\n",
    "        )\n",
    "    \n",
    "    if 'final_comparison' in locals():\n",
    "        complexity_map = {\n",
    "            'SARIMAX': 1,\n",
    "            'SVR': 2,\n",
    "            'XGBoost': 3,\n",
    "            'LSTM': 4,\n",
    "            'Hybrid ARIMA-ANN': 5\n",
    "        }\n",
    "        \n",
    "        complexity_data = []\n",
    "        for model in final_comparison['Model'].unique():\n",
    "            model_data = final_comparison[final_comparison['Model'] == model]\n",
    "            avg_r2 = model_data['R²'].mean()\n",
    "            complexity = complexity_map.get(model, 3)\n",
    "            complexity_data.append({'Model': model, 'Complexity': complexity, 'R²': avg_r2})\n",
    "        \n",
    "        complexity_df = pd.DataFrame(complexity_data)\n",
    "        \n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=complexity_df['Complexity'], y=complexity_df['R²'],\n",
    "                      mode='markers+text', text=complexity_df['Model'],\n",
    "                      textposition=\"top center\",\n",
    "                      marker=dict(size=12, color='green'),\n",
    "                      showlegend=False),\n",
    "            row=3, col=2\n",
    "        )\n",
    "    \n",
    "    fig.update_layout(\n",
    "        title_text=\"Final Comprehensive Stock Market Prediction Analysis\",\n",
    "        title_x=0.5,\n",
    "        height=1200,\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Models\", row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"R² Score\", row=1, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Actual Price\", row=1, col=2)\n",
    "    fig.update_yaxes(title_text=\"Predicted Price\", row=1, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Models\", row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"Stocks\", row=2, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Models\", row=2, col=2)\n",
    "    fig.update_yaxes(title_text=\"MSE\", row=2, col=2)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Date\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"Cumulative Returns (%)\", row=3, col=1)\n",
    "    \n",
    "    fig.update_xaxes(title_text=\"Model Complexity\", row=3, col=2)\n",
    "    fig.update_yaxes(title_text=\"R² Score\", row=3, col=2)\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "create_final_comparison_visualization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02959cbe",
   "metadata": {},
   "source": [
    "## Project Summary and Conclusions\n",
    "\n",
    "### Key Findings:\n",
    "\n",
    "1. **SVC Performance**: The Support Vector Classifier showed varying performance across different stocks, with some stocks benefiting from the SVC strategy while others performed better with buy-and-hold.\n",
    "\n",
    "2. **Model Comparison**: \n",
    "   - **SARIMAX**: Good for capturing time series patterns and seasonality\n",
    "   - **SVR**: Effective for non-linear relationships in stock price data\n",
    "   - **XGBoost**: Strong performance with ensemble learning approach\n",
    "   - **LSTM**: Excellent for capturing long-term dependencies in time series\n",
    "   - **Hybrid ARIMA-ANN**: Combines the strengths of both statistical and neural network approaches\n",
    "\n",
    "3. **Portfolio Performance**: The equal-weight portfolio created from SVC predictions showed competitive performance compared to buy-and-hold strategy.\n",
    "\n",
    "4. **Best Performing Models**: The analysis revealed which models work best for different types of stocks and market conditions.\n",
    "\n",
    "### Technical Implementation:\n",
    "\n",
    "- **Data Preprocessing**: Comprehensive data cleaning and feature engineering\n",
    "- **Time Series Split**: Proper temporal validation using TimeSeriesSplit\n",
    "- **Feature Engineering**: Created meaningful features like Open-Close, High-Low, moving averages, and volatility\n",
    "- **Model Evaluation**: Used multiple metrics (MSE, MAE, R²) for comprehensive evaluation\n",
    "- **Visualization**: Interactive Plotly charts for better understanding of results\n",
    "\n",
    "### Recommendations:\n",
    "\n",
    "1. **For Short-term Trading**: Use SVC for buy/sell decisions\n",
    "2. **For Price Prediction**: LSTM and Hybrid models show promising results\n",
    "3. **For Portfolio Management**: Consider equal-weight portfolios with model-based stock selection\n",
    "4. **For Risk Management**: Monitor model performance regularly and adjust strategies accordingly\n",
    "\n",
    "### Future Enhancements:\n",
    "\n",
    "1. **Feature Engineering**: Add more technical indicators and market sentiment data\n",
    "2. **Model Optimization**: Hyperparameter tuning for better performance\n",
    "3. **Ensemble Methods**: Combine multiple models for improved predictions\n",
    "4. **Real-time Implementation**: Deploy models for live trading decisions\n",
    "5. **Risk Management**: Implement stop-loss and position sizing strategies\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
